# GRPO Algorithm Configuration
defaults: "grpo_math_1B.yaml"

grpo:
  num_prompts_per_step: 32
  num_generations_per_prompt: 16
  max_rollout_turns: 60 # Maximum turns allowed per rollout
  max_num_steps: 10000
  normalize_rewards: false
  use_leave_one_out_baseline: false
  val_period: 10
  val_at_start: true
  max_val_samples: 512
  val_batch_size: 512

checkpointing:
  enabled: true
  checkpoint_dir: "results/grpo-infinisst"
  metric_name: "val_reward"
  higher_is_better: true
  keep_top_k: 3
  save_period: 10

policy:
  model_name: "/ckpts/infinisst/yodas_5kh_v2_m12_stage2_7b_1node/checkpoints/step=2000-last.ckpt.hf"
  # model_name: "/ckpts/llm/qwen2.5-0.5b-instruct"
  max_total_sequence_length: 2048

  dtensor_cfg:
    enabled: true
    cpu_offload: False
    sequence_parallel: false
    activation_checkpointing: false
    tensor_parallel_size: 1
    context_parallel_size: 1
    custom_parallel_plan: null

  generation:
    backend: "vllm"
    max_new_tokens: 20
    temperature: 1.0
    # Setting top_p/top_k to 0.999/10000 to strip out Qwen's special/illegal tokens
    # https://github.com/NVIDIA/NeMo-RL/issues/237
    top_p: 0.9
    top_k: null
    stop_token_ids: null
    stop_strings: null
    audio_token_id: 151656
    vllm_cfg:
      async_engine: false
      precision: ${policy.precision}
      tensor_parallel_size: 1
      pipeline_parallel_size: 1
      gpu_memory_utilization: 0.6
      max_model_len: ${policy.max_total_sequence_length}
    vllm_kwargs:
      enable_prompt_embeds: true
    colocated:
      enabled: true
      resources:
        gpus_per_node: null
        num_nodes: null

data:
  train_data_file: "/data/asr/yodas/npy/parakeet-tdt-0.6b-v2_robust60-1120_langid_zh_blaser2.0-qe3.0_metricx-qe4.0_simalign/en000/manifest.parquet"
  val_data_file: "/data/asr/yodas/npy/parakeet-tdt-0.6b-v2_robust60-1120_langid_zh_blaser2.0-qe3.0_metricx-qe4.0_simalign/en_dev/manifest.parquet"
  train_data_shuffle: true
  val_data_shuffle: false
  seed: 42
  src_lang: "en"
  tgt_lang: "zh"
 
env:
  infinisst:
    cfg:
      max_turn: ${grpo.max_rollout_turns}
      model_name: ${policy.model_name}
      scoring_model_type: "Unbabel/XCOMET-XL"
      scoring_model_path: "/ckpts/llm/xcomet-xl"
      batch_size: 8
      granularity: "whole" # "span", "sentence", "whole"
      num_gpus: 1
      
logger:
  log_dir: "logs"  # Base directory for all logs
  num_val_samples_to_print: 0 # Number of validation samples to pretty print on terminal
  wandb_enabled: false
  tensorboard_enabled: false
  monitor_gpus: true  # If true, will monitor GPU usage and log to wandb and/or tensorboard
  wandb:
    project: "grpo-dev"
    name: "grpo-dev-sliding_puzzle"
  tensorboard: {}
  gpu_monitoring:
    collection_interval: 10  # How often to collect GPU usage metrics (in seconds)
    flush_interval: 10  # How often to flush GPU usage metrics to the loggers (in seconds)

cluster:
  gpus_per_node: 1
  num_nodes: 1
